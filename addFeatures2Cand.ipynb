{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from addFeatures2Cand import ADD_covWgt, ADD_cfSim, Add_features, ADD_w2vSim, ADD_cfSim_lastN, ADD_w2vSim_lastN, ADD_covScore_lastN, ADD_covWgt_cnt, Add_freq_features\n",
    "import os\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPN_candidate = 100\n",
    "SETS = [2, 3]\n",
    "predTypes = ['orders']\n",
    "SUBSETNUM = 2\n",
    "input_note = 'covisit_20_20_20_newSuggester2'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user, item -based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_features = Add_features(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note)\n",
    "add_features.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: 2\n",
      "[Errno 17] File exists: '../output/newSplited/candidates/set2_top_100/covisit_20_20_20_newSuggester2_features_norm_freq_t0/'\n",
      "type: orders\n",
      "sub:  0\n",
      "sub:  1\n",
      "set: 3\n",
      "[Errno 17] File exists: '../output/newSplited/candidates/set3_top_100/covisit_20_20_20_newSuggester2_features_norm_freq_t0/'\n",
      "type: orders\n",
      "sub:  0\n",
      "sub:  1\n"
     ]
    }
   ],
   "source": [
    "add_features = Add_freq_features(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note)\n",
    "add_features.process()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### covisitation feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: 2\n",
      "type: orders\n",
      "sub:  0\n",
      "sub:  1\n",
      "sub:  2\n",
      "sub:  3\n",
      "sub:  4\n",
      "sub:  5\n",
      "set: 3\n",
      "type: orders\n",
      "sub:  0\n",
      "sub:  1\n",
      "sub:  2\n",
      "sub:  3\n",
      "sub:  4\n",
      "sub:  5\n"
     ]
    }
   ],
   "source": [
    "add_features = ADD_covWgt(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note, True)\n",
    "add_features.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_features = ADD_covWgt_cnt(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note, False)\n",
    "# add_features.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 2):\n",
    "    add_features = ADD_covScore_lastN(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note, False)\n",
    "    add_features.lastN = i\n",
    "    add_features.output_note = f'{add_features.output_note}_{add_features.lastN}'\n",
    "    add_features.process()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7fe7f46b7a4a95b99fd9035e979444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: orders\n",
      "sub:  0\n",
      "sub:  1\n",
      "sub:  2\n",
      "sub:  3\n",
      "sub:  4\n",
      "sub:  5\n",
      "set: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c34eb11db3f4bf18b519a4736d9495c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: orders\n",
      "sub:  0\n",
      "sub:  1\n",
      "sub:  2\n",
      "sub:  3\n",
      "sub:  4\n",
      "sub:  5\n"
     ]
    }
   ],
   "source": [
    "add_features = ADD_cfSim(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note, True)\n",
    "add_features.output_note = 'cfSim'\n",
    "add_features.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    add_features = ADD_cfSim_lastN(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note, False)\n",
    "    add_features.lastN = i\n",
    "    add_features.output_note = f'{add_features.output_note}_{add_features.lastN}'\n",
    "    add_features.process()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w2v features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_features = ADD_w2vSim(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note, True)\n",
    "add_features.output_note = 'w2vSim_3x'\n",
    "add_features.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    add_features = ADD_w2vSim_lastN(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note, False)\n",
    "    add_features.lastN = i\n",
    "    add_features.output_note = f'{add_features.output_note}_{add_features.lastN}'\n",
    "    add_features.process()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge candidate & features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates to data4xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeFeatures(feature_notes, candidates_path, predType, s, data4xgb_path):\n",
    "    for e, feature_note in enumerate(feature_notes):\n",
    "        if e == 0:\n",
    "            data4xgb = pd.read_parquet(f'{candidates_path}/{predType}_{s}.pqt').reset_index(drop=True).astype('int32')\n",
    "        else:\n",
    "            thisFeature =  pd.read_parquet(f'{candidates_path}_{feature_note}/{predType}_{s}.pqt').iloc[:, 2:].astype('float32').reset_index(drop=True)\n",
    "            data4xgb = pd.concat([data4xgb, thisFeature], axis=1)\n",
    "    data4xgb = data4xgb.fillna(0)\n",
    "    print(data4xgb.shape)\n",
    "    data4xgb.to_parquet(f'{data4xgb_path}/{predType}_{s}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: '../output/newSplited/data4xgb/set2_top_100/suggester_addLast'\n",
      "2 orders 0\n",
      "(33000000, 36)\n",
      "2 orders 1\n",
      "(33000000, 36)\n",
      "2 orders 2\n",
      "(33000000, 36)\n",
      "2 orders 3\n",
      "(33000000, 36)\n",
      "2 orders 4\n",
      "(33000000, 36)\n",
      "2 orders 5\n",
      "(15125100, 36)\n",
      "[Errno 17] File exists: '../output/newSplited/data4xgb/set3_top_100/suggester_addLast'\n",
      "3 orders 0\n",
      "(33000000, 35)\n",
      "3 orders 1\n",
      "(33000000, 35)\n",
      "3 orders 2\n",
      "(33000000, 35)\n",
      "3 orders 3\n",
      "(33000000, 35)\n",
      "3 orders 4\n",
      "(33000000, 35)\n",
      "3 orders 5\n",
      "(2180324, 35)\n"
     ]
    }
   ],
   "source": [
    "outputPath = '../output/newSplited/'\n",
    "# feature_notes = ['', 'features_dropped', 'covWgt_t1', 'cfSim_last_1_t0', 'cfSim_last_2_t0', 'cfSim_last_3_t0', 'w2vSim_last_1_t0', 'w2vSim_last_2_t0', 'w2vSim_last_3_t0', 'w2vSim_last_4_t0', 'covScore_last_1_t0']\n",
    "feature_notes = ['', 'features_dropped', 'covWgt_t1', 'covScore_last_1_t0'] #'cfSim_last_1_t0', 'cfSim_last_2_t0', 'cfSim_last_3_t0', 'w2vSim_last_1_t0', 'w2vSim_last_2_t0', 'w2vSim_last_3_t0', 'w2vSim_last_4_t0', ]\n",
    "data4xgb_note = 'suggester_addLast'\n",
    "\n",
    "for SET in SETS:\n",
    "    candidates_path = outputPath + f'candidates/set{SET}_top_{TOPN_candidate}/{input_note}'\n",
    "    data4xgb_path = outputPath + f'data4xgb/set{SET}_top_{TOPN_candidate}/{data4xgb_note}'\n",
    "    try: \n",
    "        os.makedirs(data4xgb_path) \n",
    "    except OSError as error: \n",
    "        print(error)\n",
    "\n",
    "    for predType in predTypes:\n",
    "        for s in range(SUBSETNUM):\n",
    "            print(SET, predType, s)\n",
    "            mergeFeatures(feature_notes, candidates_path, predType, s, data4xgb_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data4xgb to data4xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFeatures(feature_notes, input_path, predType, s, data4xgb_path, output_path):\n",
    "    # if predType == 'orders':\n",
    "        # toDrop = ['covScore_cartsOrders_2', 'covScore_buy2buy_2', 'covScore_cartsOrders_3', 'covScore_buy2buy_3', 'covScore_buy2buy_1', 'item_ts_min_valA', 'item_type_std_valA']\n",
    "    data4xgb = pd.read_parquet(f'{data4xgb_path}/{predType}_{s}.pqt').reset_index(drop=True)#.drop(columns=toDrop)\n",
    "    \n",
    "    for feature_note in feature_notes:\n",
    "        thisFeature =  pd.read_parquet(f'{input_path}_{feature_note}/{predType}_{s}.pqt').iloc[:, 2:].astype('float32').reset_index(drop=True)\n",
    "        data4xgb = pd.concat([data4xgb, thisFeature], axis=1)\n",
    "    data4xgb.fillna(0)\n",
    "    print(data4xgb.shape)\n",
    "    # data4xgb.to_parquet(f'{output_path}/{predType}_{s}.pqt')\n",
    "\n",
    "    chunkSize = 33000000\n",
    "    idx = 0\n",
    "    n = len(data4xgb)\n",
    "    while (idx * chunkSize < n):\n",
    "        # dtest = xgb.DMatrix(data4xgb.iloc[i*chunkSize:(i+1)*chunkSize, 2:])\n",
    "        sub = data4xgb[idx*chunkSize:(idx+1)*chunkSize].reset_index(drop=True)\n",
    "        sub.to_parquet(f'{output_path}/{predType}_{s}_{idx}.pqt')\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: '../output/newSplited/data4xgb/set2_top_100/covisit_20_20_20_newSuggester2_add_freq'\n",
      "2 orders 0\n",
      "(90062550, 50)\n",
      "2 orders 1\n",
      "(90062550, 50)\n",
      "[Errno 17] File exists: '../output/newSplited/data4xgb/set3_top_100/covisit_20_20_20_newSuggester2_add_freq'\n",
      "3 orders 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(SUBSETNUM):\n\u001b[1;32m     18\u001b[0m     \u001b[39mprint\u001b[39m(SET, predType, s)\n\u001b[0;32m---> 19\u001b[0m     addFeatures(feature_notes, input_path, predType, s, data4xgb_path, output_path)\n",
      "Cell \u001b[0;32mIn [6], line 4\u001b[0m, in \u001b[0;36maddFeatures\u001b[0;34m(feature_notes, input_path, predType, s, data4xgb_path, output_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maddFeatures\u001b[39m(feature_notes, input_path, predType, s, data4xgb_path, output_path):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# if predType == 'orders':\u001b[39;00m\n\u001b[1;32m      3\u001b[0m         \u001b[39m# toDrop = ['covScore_cartsOrders_2', 'covScore_buy2buy_2', 'covScore_cartsOrders_3', 'covScore_buy2buy_3', 'covScore_buy2buy_1', 'item_ts_min_valA', 'item_type_std_valA']\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     data4xgb \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_parquet(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mdata4xgb_path\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mpredType\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00ms\u001b[39m}\u001b[39;49;00m\u001b[39m.pqt\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m#.drop(columns=toDrop)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[39mfor\u001b[39;00m feature_note \u001b[39min\u001b[39;00m feature_notes:\n\u001b[1;32m      7\u001b[0m         thisFeature \u001b[39m=\u001b[39m  pd\u001b[39m.\u001b[39mread_parquet(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00minput_path\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mfeature_note\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mpredType\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m.pqt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39miloc[:, \u001b[39m2\u001b[39m:]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pandas/io/parquet.py:503\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[39mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39mDataFrame\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    501\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    504\u001b[0m     path,\n\u001b[1;32m    505\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m    506\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    507\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[1;32m    508\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    509\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pandas/io/parquet.py:251\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m path_or_handle, handles, kwargs[\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    245\u001b[0m     path,\n\u001b[1;32m    246\u001b[0m     kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[1;32m    248\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    249\u001b[0m )\n\u001b[1;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mparquet\u001b[39m.\u001b[39;49mread_table(\n\u001b[1;32m    252\u001b[0m         path_or_handle, columns\u001b[39m=\u001b[39;49mcolumns, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    253\u001b[0m     )\u001b[39m.\u001b[39mto_pandas(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mto_pandas_kwargs)\n\u001b[1;32m    254\u001b[0m     \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    255\u001b[0m         result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39m_as_manager(\u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyarrow/parquet/__init__.py:2827\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[1;32m   2816\u001b[0m         \u001b[39m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[1;32m   2817\u001b[0m         dataset \u001b[39m=\u001b[39m ParquetFile(\n\u001b[1;32m   2818\u001b[0m             source, metadata\u001b[39m=\u001b[39mmetadata, read_dictionary\u001b[39m=\u001b[39mread_dictionary,\n\u001b[1;32m   2819\u001b[0m             memory_map\u001b[39m=\u001b[39mmemory_map, buffer_size\u001b[39m=\u001b[39mbuffer_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2824\u001b[0m             thrift_container_size_limit\u001b[39m=\u001b[39mthrift_container_size_limit,\n\u001b[1;32m   2825\u001b[0m         )\n\u001b[0;32m-> 2827\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\u001b[39m.\u001b[39;49mread(columns\u001b[39m=\u001b[39;49mcolumns, use_threads\u001b[39m=\u001b[39;49muse_threads,\n\u001b[1;32m   2828\u001b[0m                         use_pandas_metadata\u001b[39m=\u001b[39;49muse_pandas_metadata)\n\u001b[1;32m   2830\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2831\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPassing \u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_legacy_dataset=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to get the legacy behaviour is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2832\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdeprecated as of pyarrow 8.0.0, and the legacy implementation will \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2833\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbe removed in a future version.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2834\u001b[0m     \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m   2836\u001b[0m \u001b[39mif\u001b[39;00m ignore_prefixes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyarrow/parquet/__init__.py:2473\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.read\u001b[0;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[1;32m   2465\u001b[0m         index_columns \u001b[39m=\u001b[39m [\n\u001b[1;32m   2466\u001b[0m             col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[1;32m   2467\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, \u001b[39mdict\u001b[39m)\n\u001b[1;32m   2468\u001b[0m         ]\n\u001b[1;32m   2469\u001b[0m         columns \u001b[39m=\u001b[39m (\n\u001b[1;32m   2470\u001b[0m             \u001b[39mlist\u001b[39m(columns) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(index_columns) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(columns))\n\u001b[1;32m   2471\u001b[0m         )\n\u001b[0;32m-> 2473\u001b[0m table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset\u001b[39m.\u001b[39;49mto_table(\n\u001b[1;32m   2474\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_filter_expression,\n\u001b[1;32m   2475\u001b[0m     use_threads\u001b[39m=\u001b[39;49muse_threads\n\u001b[1;32m   2476\u001b[0m )\n\u001b[1;32m   2478\u001b[0m \u001b[39m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[1;32m   2479\u001b[0m \u001b[39m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[1;32m   2480\u001b[0m \u001b[39mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "outputPath = '../output/newSplited/'\n",
    "# feature_notes = ['cfSim_3x_t1', 'cfSim_last_1_t0', 'cfSim_last_2_t0', 'cfSim_last_3_t0'] \n",
    "# feature_notes =  ['w2vSim_3x_t1', 'w2vSim_last_1_t0', 'w2vSim_last_2_t0', 'w2vSim_last_3_t0', 'w2vSim_last_4_t0']\n",
    "feature_notes = ['features_norm_freq_t0']\n",
    "data4xgb_note = 'covisit_20_20_20_newSuggester2_drop_12_add_last3CovScore'\n",
    "output_note = 'covisit_20_20_20_newSuggester2_add_freq'\n",
    "for SET in SETS:\n",
    "    input_path = outputPath + f'candidates/set{SET}_top_{TOPN_candidate}/{input_note}'\n",
    "    data4xgb_path = outputPath + f'data4xgb/set{SET}_top_{TOPN_candidate}/{data4xgb_note}'\n",
    "    output_path = outputPath + f'data4xgb/set{SET}_top_{TOPN_candidate}/{output_note}'\n",
    "    try: \n",
    "        os.makedirs(output_path) \n",
    "    except OSError as error: \n",
    "        print(error)\n",
    "\n",
    "    for predType in predTypes:\n",
    "        for s in range(SUBSETNUM):\n",
    "            print(SET, predType, s)\n",
    "            addFeatures(feature_notes, input_path, predType, s, data4xgb_path, output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns from data4xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../output/newSplited/data4xgb/set2_top_100/suggester_addLast’: File exists\n",
      "mkdir: cannot create directory ‘../output/newSplited/data4xgb/set3_top_100/suggester_addLast’: File exists\n"
     ]
    }
   ],
   "source": [
    "input_note = 'suggester_addLast'\n",
    "output_note = 'suggester_addLast'\n",
    "\n",
    "for SET in [2, 3]:\n",
    "    ! mkdir ../output/newSplited/data4xgb/set{SET}_top_100/{output_note}\n",
    "    for predType in predTypes:\n",
    "        if predType == 'clicks':\n",
    "            toDrop_col = ['item_type_median', 'item_type_median_valA', 'item_clicked_cnt_val', 'item_carted_cnt_val', 'item_ordered_cnt_val', 'user_type_median']\n",
    "        elif predType == 'carts':\n",
    "            toDrop_col = ['item_type_median', 'item_type_median_valA', 'item_clicked_cnt_val', 'item_carted_cnt_val', 'item_ordered_cnt_val', 'user_type_median']\n",
    "        elif predType == 'orders':\n",
    "            # toDrop_col = ['item_type_median', 'item_type_median_valA', 'item_clicked_cnt_val', 'item_carted_cnt_val', 'item_ordered_cnt_val', 'user_type_median']\n",
    "            # toDrop_col = ['item_type_median_valA', 'item_type_median', 'user_type_median', 'item_clicked_cnt_val', 'item_ordered_cnt_val']\n",
    "            toDrop_col = ['covScore_buy2buy_1', 'wgt_buy2buy', 'user_ts_diff_std', 'item_carted_cnt_val', 'user_ts_min']\n",
    "\n",
    "            \n",
    "        for sub in range(SUBSETNUM):\n",
    "            data4xgb = pd.read_parquet(f'../output/newSplited/data4xgb/set{SET}_top_100/{input_note}/{predType}_{sub}.pqt').drop(toDrop_col, axis=1)\n",
    "            data4xgb.to_parquet(f'../output/newSplited/data4xgb/set{SET}_top_100/{output_note}/{predType}_{sub}.pqt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns from candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_toDrop_col(predType):\n",
    "    if predType == 'clicks':\n",
    "        toDrop_col = ['user_ordered_cnt', 'cnt_ordered', 'user_lastAid', 'user_clicked_cnt', 'user_carted_cnt', 'item_ts_min_valA', 'user_type_std', 'user_ts_min', 'item_buy_ratio_valA', 'item_type_std_valA', 'item_buy_ratio', 'item_type_std']\n",
    "    elif predType == 'carts':\n",
    "        toDrop_col = ['user_ordered_cnt', 'cnt_ordered', 'user_clicked_cnt', 'item_item_count', 'user_user_count', 'item_carted_cnt', 'user_type_std', 'item_user_count_valA', 'user_ts_min', 'item_ordered_cnt', 'item_ts_min_valA', 'user_lastAid']\n",
    "    elif predType == 'orders':\n",
    "        toDrop_col = ['user_ordered_cnt', 'item_user_count_valA', 'user_lastAid', 'item_item_count', 'user_ts_mean', 'user_clicked_cnt', 'item_clicked_cnt', 'item_item_count_valA', 'user_type_std', 'item_ts_min', 'user_user_count', 'item_ts_min_valA', 'item_type_std_valA']\n",
    "    return toDrop_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_note = 'features_norm_addLast_t0'\n",
    "output_note = 'features_dropped'\n",
    "\n",
    "for SET in [2, 3]:\n",
    "    ! mkdir ../output/newSplited/candidates/set{SET}_top_100/suggester_addLast_{output_note}\n",
    "    for predType in predTypes:\n",
    "        toDrop_col = load_toDrop_col(predType)\n",
    "        \n",
    "        for sub in range(SUBSETNUM):\n",
    "            data4xgb = pd.read_parquet(f'../output/newSplited/candidates/set{SET}_top_100/suggester_addLast_{input_note}/{predType}_{sub}.pqt').drop(toDrop_col, axis=1)\n",
    "            data4xgb.to_parquet(f'../output/newSplited/candidates/set{SET}_top_100/suggester_addLast_{output_note}/{predType}_{sub}.pqt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user-based & item-based features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../output/newSplited/candidates/set2_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0’: File exists\n",
      "mkdir: cannot create directory ‘../output/newSplited/candidates/set2_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0’: File exists\n",
      "mkdir: cannot create directory ‘../output/newSplited/candidates/set2_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0’: File exists\n",
      "mkdir: cannot create directory ‘../output/newSplited/candidates/set2_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0’: File exists\n",
      "mkdir: cannot create directory ‘../output/newSplited/candidates/set2_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0’: File exists\n",
      "mkdir: cannot create directory ‘../output/newSplited/candidates/set3_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0’: File exists\n",
      "mkdir: cannot create directory ‘../output/newSplited/candidates/set3_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0’: File exists\n",
      "mkdir: cannot create directory ‘../output/newSplited/candidates/set3_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0’: File exists\n",
      "mkdir: cannot create directory ‘../output/newSplited/candidates/set3_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0’: File exists\n",
      "mkdir: cannot create directory ‘../output/newSplited/candidates/set3_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0’: File exists\n"
     ]
    }
   ],
   "source": [
    "toDrop_col = ['item_type_median', 'item_type_median_valA', 'item_clicked_cnt_val', 'item_carted_cnt_val', 'item_ordered_cnt_val', 'user_type_median']\n",
    "\n",
    "for SET in [2, 3]:\n",
    "    ! mkdir ../output/newSplited/candidates/set{SET}_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0\n",
    "    for predType in ['clicks', 'carts', 'orders']:\n",
    "        for sub in range(2):\n",
    "            data4xgb = pd.read_parquet(f'../output/newSplited/candidates/set{SET}_top_100/covisit_20_20_20_newSuggester2_features_norm_addLast_t0/{predType}_{sub}.pqt').drop(toDrop_col, axis=1)\n",
    "            data4xgb.to_parquet(f'../output/newSplited/candidates/set{SET}_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0/{predType}_{sub}.pqt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add item similarity with last item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 clicks 0\n",
      "2 clicks 1\n",
      "2 carts 0\n",
      "2 carts 1\n",
      "2 orders 0\n",
      "2 orders 1\n",
      "3 clicks 0\n",
      "3 clicks 1\n",
      "3 carts 0\n",
      "3 carts 1\n",
      "3 orders 0\n",
      "3 orders 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aid</th>\n",
       "      <th>user_lastAid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11830</td>\n",
       "      <td>11830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>588923</td>\n",
       "      <td>11830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1732105</td>\n",
       "      <td>11830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>571762</td>\n",
       "      <td>11830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>884502</td>\n",
       "      <td>11830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196637084</th>\n",
       "      <td>868327</td>\n",
       "      <td>519105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196637085</th>\n",
       "      <td>406358</td>\n",
       "      <td>519105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196637086</th>\n",
       "      <td>269257</td>\n",
       "      <td>519105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196637087</th>\n",
       "      <td>272744</td>\n",
       "      <td>519105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196637088</th>\n",
       "      <td>165160</td>\n",
       "      <td>519105.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196637089 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               aid  user_lastAid\n",
       "0            11830       11830.0\n",
       "1           588923       11830.0\n",
       "2          1732105       11830.0\n",
       "3           571762       11830.0\n",
       "4           884502       11830.0\n",
       "...            ...           ...\n",
       "196637084   868327      519105.0\n",
       "196637085   406358      519105.0\n",
       "196637086   269257      519105.0\n",
       "196637087   272744      519105.0\n",
       "196637088   165160      519105.0\n",
       "\n",
       "[196637089 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toConcat = []\n",
    "for SET in [2, 3]:\n",
    "    for predType in ['clicks', 'carts', 'orders']:\n",
    "        for sub in range(2):\n",
    "            print(SET, predType, sub)\n",
    "            data4xgb = pd.read_parquet(f'../output/newSplited/candidates/set{SET}_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0/{predType}_{sub}.pqt')[['aid', 'user_lastAid']]\n",
    "            toConcat.append(data4xgb)\n",
    "data4xgb = pd.concat(toConcat, axis=0, ignore_index=True)\n",
    "data4xgb = data4xgb.drop_duplicates().reset_index(drop=True)\n",
    "data4xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "aid2vec_model = gensim.models.KeyedVectors.load_word2vec_format('../output/newSplited/savedModel/otto_aid2vec_5d.bin', binary=True)\n",
    "cf_itemSimMatrix = np.load(f'../output/newSplited/cf_matrix/set_{2}/itemSimMatrix.npy', allow_pickle='TRUE').item()\n",
    "\n",
    "tqdm.pandas()\n",
    "def cf_smilarity(x):\n",
    "    return aid2vec_model.similarity(str(x.aid), str(x.user_lastAid))\n",
    "    # try:\n",
    "    #     return [cf_itemSimMatrix[x.aid][x.user_lastAid], aid2vec_model.similarity(str(x.aid), str(x.user_lastAid))]\n",
    "    # except:\n",
    "    #     return [0, aid2vec_model.similarity(str(x.aid), str(x.user_lastAid))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4xgb['user_lastAid'] = data4xgb['user_lastAid'].astype('int32')\n",
    "data4xgb['w2v_sim_last'] = data4xgb.progress_apply(lambda x: cf_smilarity(x), axis=1)\n",
    "\n",
    "# data4xgb[['cf_sim_last', 'w2v_sim_last']] = pd.DataFrame(data4xgb.progress_apply(lambda x: cf_smilarity(x), axis=1).to_list())\n",
    "data4xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4xgb.to_parquet('../output/newSplited/features/aid2aid_similarity.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for SET in [2, 3]:\n",
    "    ! mkdir ../output/newSplited/candidates/set{SET}_top_100/covisit_20_20_20_newSuggester2_features_norm_addLastSim_t0\n",
    "    for predType in ['clicks', 'carts', 'orders']:\n",
    "        for sub in range(2):\n",
    "            print(SET, predType, sub)\n",
    "            candidates = pd.read_parquet(f'../output/newSplited/candidates/set{SET}_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0/{predType}_{sub}.pqt')\n",
    "            candidates = candidates.merge(data4xgb, on=['aid', 'user_lastAid'], how='left')\n",
    "            candidates.to_parquet(f'../output/newSplited/candidates/set{SET}_top_100/covisit_20_20_20_newSuggester2_features_norm_addLastSim_t0/{predType}_{sub}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../output/newSplited/candidates/set2_top_100/covisit_20_20_20_newSuggester2_features_norm_addLastSim_t0’: File exists\n",
      "2\n",
      "carts\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f8c0d5a3764f1dbe0cdb5e59a8289a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95426d04a47d41ffa8f438fcb69459f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88dc420549c4ec8ae04c641242dfe17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b192a089bb0425bae03e829c3183569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35758581841e4a63aed86778b106a0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c076d3070d9249468359a49b09e30985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "carts\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913247a4964f4affa30b2e36ac6284bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c864027ddb374b4cb5b335eeb8185c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a37df7ce44a4b6da03a8264d512ee29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f5a400a45845a3b3f55ab41467cf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8731a859a6c4fe9b068266e51077651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86dc86f331f4cc5a1658a0688ac00cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim\n",
    "aid2vec_model = gensim.models.KeyedVectors.load_word2vec_format('../output/newSplited/savedModel/otto_aid2vec_5d.bin', binary=True)\n",
    "tqdm.pandas()\n",
    "\n",
    "def cf_smilarity(x):\n",
    "    try:\n",
    "        return [cf_itemSimMatrix[x.aid][x.user_lastAid], aid2vec_model.similarity(str(x.aid), str(x.user_lastAid))]\n",
    "    except:\n",
    "        return [0, aid2vec_model.similarity(str(x.aid), str(x.user_lastAid))]\n",
    "\n",
    "for SET in [2, 3]:\n",
    "    ! mkdir ../output/newSplited/candidates/set{SET}_top_100/covisit_20_20_20_newSuggester2_features_norm_addLastSim_t0\n",
    "    print(SET)\n",
    "    cf_itemSimMatrix = np.load(f'../output/newSplited/cf_matrix/set_{SET}/itemSimMatrix.npy', allow_pickle='TRUE').item()\n",
    "    for predType in ['carts', 'orders', 'clicks']:\n",
    "        print(predType)\n",
    "        for sub in range(2):\n",
    "            print(sub)\n",
    "            data4xgb = pd.read_parquet(f'../output/newSplited/candidates/set{SET}_top_100/covisit_20_20_20_newSuggester2_features_norm_cutBadFeatures_t0/{predType}_{sub}.pqt')\n",
    "\n",
    "            aidx_aidy = data4xgb[['aid', 'user_lastAid']].drop_duplicates().reset_index(drop=True)\n",
    "            aidx_aidy['user_lastAid'] = aidx_aidy['user_lastAid'].astype('int32')\n",
    "            aidx_aidy[['cf_sim_last', 'w2v_sim_last']] = pd.DataFrame(aidx_aidy.progress_apply(lambda x: cf_smilarity(x), axis=1).to_list())\n",
    "            data4xgb = data4xgb.merge(aidx_aidy, on = ['aid', 'user_lastAid'], how='left')\n",
    "\n",
    "            data4xgb.to_parquet(f'../output/newSplited/candidates/set{SET}_top_100/covisit_20_20_20_newSuggester2_features_norm_addLastSim_t0/{predType}_{sub}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f708a36acfaef0acf74ccd43dfb58100269bf08fb79032a1e0a6f35bd9856f51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
