{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from functions.featuresAdder import ADD_covWgt, ADD_cfSim, Add_features, ADD_w2vSim, ADD_cfSim_lastN, ADD_w2vSim_lastN, ADD_covScore_lastN, Add_freq_features\n",
    "import os\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPN_candidate = 100\n",
    "SETS = [2, 3]\n",
    "predTypes = ['clicks', 'carts', 'orders']\n",
    "SUBSETNUM = 2\n",
    "input_note = 'covisit_20_20_20_newSuggester2'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add features to candidates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user, item -based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_features = Add_features(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note)\n",
    "add_features.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_features = Add_freq_features(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note)\n",
    "add_features.process()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### covisitation feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_features = ADD_covWgt(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note, True)\n",
    "add_features.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 2):\n",
    "    add_features = ADD_covScore_lastN(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note, False)\n",
    "    add_features.lastN = i\n",
    "    add_features.output_note = f'{add_features.output_note}_{add_features.lastN}'\n",
    "    add_features.process()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_features = ADD_cfSim(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note, True)\n",
    "add_features.output_note = 'cfSim'\n",
    "add_features.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    add_features = ADD_cfSim_lastN(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note, False)\n",
    "    add_features.lastN = i\n",
    "    add_features.output_note = f'{add_features.output_note}_{add_features.lastN}'\n",
    "    add_features.process()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w2v features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_features = ADD_w2vSim(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note, True)\n",
    "add_features.output_note = 'w2vSim_3x'\n",
    "add_features.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    add_features = ADD_w2vSim_lastN(TOPN_candidate, SETS, predTypes, SUBSETNUM, input_note, False)\n",
    "    add_features.lastN = i\n",
    "    add_features.output_note = f'{add_features.output_note}_{add_features.lastN}'\n",
    "    add_features.process()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge candidate & features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates to data4xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeFeatures(feature_notes, candidates_path, predType, s, data4xgb_path):\n",
    "    for e, feature_note in enumerate(feature_notes):\n",
    "        if e == 0:\n",
    "            data4xgb = pd.read_parquet(f'{candidates_path}/{predType}_{s}.pqt').reset_index(drop=True).astype('int32')\n",
    "        else:\n",
    "            thisFeature =  pd.read_parquet(f'{candidates_path}_{feature_note}/{predType}_{s}.pqt').iloc[:, 2:].astype('float32').reset_index(drop=True)\n",
    "            data4xgb = pd.concat([data4xgb, thisFeature], axis=1)\n",
    "    data4xgb = data4xgb.fillna(0)\n",
    "    print(data4xgb.shape)\n",
    "    data4xgb.to_parquet(f'{data4xgb_path}/{predType}_{s}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = '../output/newSplited/'\n",
    "# feature_notes = ['', 'features_dropped', 'covWgt_t1', 'cfSim_last_1_t0', 'cfSim_last_2_t0', 'cfSim_last_3_t0', 'w2vSim_last_1_t0', 'w2vSim_last_2_t0', 'w2vSim_last_3_t0', 'w2vSim_last_4_t0', 'covScore_last_1_t0']\n",
    "feature_notes = ['', 'features_dropped', 'covWgt_t1', 'covScore_last_1_t0'] #'cfSim_last_1_t0', 'cfSim_last_2_t0', 'cfSim_last_3_t0', 'w2vSim_last_1_t0', 'w2vSim_last_2_t0', 'w2vSim_last_3_t0', 'w2vSim_last_4_t0', ]\n",
    "data4xgb_note = 'suggester_addLast'\n",
    "\n",
    "for SET in SETS:\n",
    "    candidates_path = outputPath + f'candidates/set{SET}_top_{TOPN_candidate}/{input_note}'\n",
    "    data4xgb_path = outputPath + f'data4xgb/set{SET}_top_{TOPN_candidate}/{data4xgb_note}'\n",
    "    try: \n",
    "        os.makedirs(data4xgb_path) \n",
    "    except OSError as error: \n",
    "        print(error)\n",
    "\n",
    "    for predType in predTypes:\n",
    "        for s in range(SUBSETNUM):\n",
    "            print(SET, predType, s)\n",
    "            mergeFeatures(feature_notes, candidates_path, predType, s, data4xgb_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data4xgb to data4xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFeatures(feature_notes, input_path, predType, s, data4xgb_path, output_path):\n",
    "    # if predType == 'orders':\n",
    "        # toDrop = ['covScore_cartsOrders_2', 'covScore_buy2buy_2', 'covScore_cartsOrders_3', 'covScore_buy2buy_3', 'covScore_buy2buy_1', 'item_ts_min_valA', 'item_type_std_valA']\n",
    "    data4xgb = pd.read_parquet(f'{data4xgb_path}/{predType}_{s}.pqt').reset_index(drop=True)#.drop(columns=toDrop)\n",
    "    \n",
    "    for feature_note in feature_notes:\n",
    "        thisFeature =  pd.read_parquet(f'{input_path}_{feature_note}/{predType}_{s}.pqt').iloc[:, 2:].astype('float32').reset_index(drop=True)\n",
    "        data4xgb = pd.concat([data4xgb, thisFeature], axis=1)\n",
    "    data4xgb.fillna(0)\n",
    "    print(data4xgb.shape)\n",
    "    # data4xgb.to_parquet(f'{output_path}/{predType}_{s}.pqt')\n",
    "\n",
    "    chunkSize = 33000000\n",
    "    idx = 0\n",
    "    n = len(data4xgb)\n",
    "    while (idx * chunkSize < n):\n",
    "        # dtest = xgb.DMatrix(data4xgb.iloc[i*chunkSize:(i+1)*chunkSize, 2:])\n",
    "        sub = data4xgb[idx*chunkSize:(idx+1)*chunkSize].reset_index(drop=True)\n",
    "        sub.to_parquet(f'{output_path}/{predType}_{s}_{idx}.pqt')\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = '../output/newSplited/'\n",
    "# feature_notes = ['cfSim_3x_t1', 'cfSim_last_1_t0', 'cfSim_last_2_t0', 'cfSim_last_3_t0'] \n",
    "# feature_notes =  ['w2vSim_3x_t1', 'w2vSim_last_1_t0', 'w2vSim_last_2_t0', 'w2vSim_last_3_t0', 'w2vSim_last_4_t0']\n",
    "feature_notes = ['features_norm_freq_t0']\n",
    "data4xgb_note = 'covisit_20_20_20_newSuggester2_drop_12_add_last3CovScore'\n",
    "output_note = 'covisit_20_20_20_newSuggester2_add_freq'\n",
    "for SET in SETS:\n",
    "    input_path = outputPath + f'candidates/set{SET}_top_{TOPN_candidate}/{input_note}'\n",
    "    data4xgb_path = outputPath + f'data4xgb/set{SET}_top_{TOPN_candidate}/{data4xgb_note}'\n",
    "    output_path = outputPath + f'data4xgb/set{SET}_top_{TOPN_candidate}/{output_note}'\n",
    "    try: \n",
    "        os.makedirs(output_path) \n",
    "    except OSError as error: \n",
    "        print(error)\n",
    "\n",
    "    for predType in predTypes:\n",
    "        for s in range(SUBSETNUM):\n",
    "            print(SET, predType, s)\n",
    "            addFeatures(feature_notes, input_path, predType, s, data4xgb_path, output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop useless features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns from data4xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../output/newSplited/data4xgb/set2_top_100/suggester_addLast’: File exists\n",
      "mkdir: cannot create directory ‘../output/newSplited/data4xgb/set3_top_100/suggester_addLast’: File exists\n"
     ]
    }
   ],
   "source": [
    "input_note = 'suggester_addLast'\n",
    "output_note = 'suggester_addLast'\n",
    "\n",
    "for SET in [2, 3]:\n",
    "    ! mkdir ../output/newSplited/data4xgb/set{SET}_top_100/{output_note}\n",
    "    for predType in predTypes:\n",
    "        if predType == 'clicks':\n",
    "            toDrop_col = ['item_type_median', 'item_type_median_valA', 'item_clicked_cnt_val', 'item_carted_cnt_val', 'item_ordered_cnt_val', 'user_type_median']\n",
    "        elif predType == 'carts':\n",
    "            toDrop_col = ['item_type_median', 'item_type_median_valA', 'item_clicked_cnt_val', 'item_carted_cnt_val', 'item_ordered_cnt_val', 'user_type_median']\n",
    "        elif predType == 'orders':\n",
    "            # toDrop_col = ['item_type_median', 'item_type_median_valA', 'item_clicked_cnt_val', 'item_carted_cnt_val', 'item_ordered_cnt_val', 'user_type_median']\n",
    "            # toDrop_col = ['item_type_median_valA', 'item_type_median', 'user_type_median', 'item_clicked_cnt_val', 'item_ordered_cnt_val']\n",
    "            toDrop_col = ['covScore_buy2buy_1', 'wgt_buy2buy', 'user_ts_diff_std', 'item_carted_cnt_val', 'user_ts_min']\n",
    "\n",
    "            \n",
    "        for sub in range(SUBSETNUM):\n",
    "            data4xgb = pd.read_parquet(f'../output/newSplited/data4xgb/set{SET}_top_100/{input_note}/{predType}_{sub}.pqt').drop(toDrop_col, axis=1)\n",
    "            data4xgb.to_parquet(f'../output/newSplited/data4xgb/set{SET}_top_100/{output_note}/{predType}_{sub}.pqt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns from candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_toDrop_col(predType):\n",
    "    if predType == 'clicks':\n",
    "        toDrop_col = ['user_ordered_cnt', 'cnt_ordered', 'user_lastAid', 'user_clicked_cnt', 'user_carted_cnt', 'item_ts_min_valA', 'user_type_std', 'user_ts_min', 'item_buy_ratio_valA', 'item_type_std_valA', 'item_buy_ratio', 'item_type_std']\n",
    "    elif predType == 'carts':\n",
    "        toDrop_col = ['user_ordered_cnt', 'cnt_ordered', 'user_clicked_cnt', 'item_item_count', 'user_user_count', 'item_carted_cnt', 'user_type_std', 'item_user_count_valA', 'user_ts_min', 'item_ordered_cnt', 'item_ts_min_valA', 'user_lastAid']\n",
    "    elif predType == 'orders':\n",
    "        toDrop_col = ['user_ordered_cnt', 'item_user_count_valA', 'user_lastAid', 'item_item_count', 'user_ts_mean', 'user_clicked_cnt', 'item_clicked_cnt', 'item_item_count_valA', 'user_type_std', 'item_ts_min', 'user_user_count', 'item_ts_min_valA', 'item_type_std_valA']\n",
    "    return toDrop_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_note = 'features_norm_addLast_t0'\n",
    "output_note = 'features_dropped'\n",
    "\n",
    "for SET in [2, 3]:\n",
    "    ! mkdir ../output/newSplited/candidates/set{SET}_top_100/suggester_addLast_{output_note}\n",
    "    for predType in predTypes:\n",
    "        toDrop_col = load_toDrop_col(predType)\n",
    "        \n",
    "        for sub in range(SUBSETNUM):\n",
    "            data4xgb = pd.read_parquet(f'../output/newSplited/candidates/set{SET}_top_100/suggester_addLast_{input_note}/{predType}_{sub}.pqt').drop(toDrop_col, axis=1)\n",
    "            data4xgb.to_parquet(f'../output/newSplited/candidates/set{SET}_top_100/suggester_addLast_{output_note}/{predType}_{sub}.pqt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f708a36acfaef0acf74ccd43dfb58100269bf08fb79032a1e0a6f35bd9856f51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
